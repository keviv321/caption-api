# -*- coding: utf-8 -*-
"""Attention8kFinal.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vcVfm_yEYXlzR741brh318hQlaEBuPHh
"""

import tensorflow as tf
import matplotlib.pyplot as plt
import numpy as np
from PIL  import Image
import pickle

#from google.colab import drive
#drive.mount('/content/drive')

#Encoder architecture
#on the top of existing InceptionV3 model architecture we are adding one more
#layer in which embedding_dim(256) neuron is and only this layer weight will update not rest of the layer

class CNN_Encoder(tf.keras.Model):
  def __init__(self, embedding_dim):
    super(CNN_Encoder, self).__init__()
    #from inceptionv3 model we got (batch_size, 64, 2048)
    #embedding_dim - output space dimension i.e. no. of neuron
    #after applying this shape will be (batch_size, 64, embedding_dim)
    #embedding_dim ---> 256
    self.fc = tf.keras.layers.Dense(embedding_dim)

  def call(self, x):
    x = self.fc(x)
    #applying relu on top of x
    x = tf.nn.relu(x)
    return x

class Attention(tf.keras.Model):

  def __init__(self, units):
    super(Attention, self).__init__()
    #this weight used for annotation vector i.e. image feature
    #output space dimension - units(512)
    self.W1 = tf.keras.layers.Dense(units)
    #this weight used for hidden state of lstm/gru
    #output space dimension - units(512)
    self.W2 = tf.keras.layers.Dense(units)
    #used to obtain attention weight
    self.V = tf.keras.layers.Dense(1)

  def call(self, feature, hidden):
    # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)

    # hidden shape == (batch_size, hidden_size)
    # hidden_with_time_axis shape == (batch_size, 1, hidden_size)
    hidden_with_time_axis = tf.expand_dims(hidden, 1)

    # score shape == (batch_size, 64, hidden_size)
    #hidden_size---->units(same shape my asssumption)
    score = tf.nn.tanh(self.W1(feature) + self.W2(hidden_with_time_axis))
    #shape of attention weight is (batch_size, 64, 1)
    #axis = 1: apply on row
    attention_weights = tf.nn.softmax(self.V(score), axis=1)

    # context_vector shape after sum == (batch_size, hidden_size)
    context_vector = attention_weights * feature
    context_vector = tf.reduce_sum(context_vector, axis=1)

    return context_vector, attention_weights

class RNN_Decoder(tf.keras.Model):

  def __init__(self, embedding_dim, units, vocab_size):
    super(RNN_Decoder, self).__init__()
    self.units = units
    
    #Size of the vocabulary --> vocab_size(input size)
    #Dimension of the dense embedding -->embedding_dim(output size)
    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)
    self.gru = tf.keras.layers.GRU(self.units, return_sequences=True, return_state=True, recurrent_initializer="glorot_uniform")
    #used to convert output of gru to units size
    self.fc1 = tf.keras.layers.Dense(self.units)
    #used to convert output to vocab size to get the word
    self.fc2 = tf.keras.layers.Dense(vocab_size)

    #attention layer between encoder and decoder
    self.attention = Attention(self.units)
  
  def call(self, x, feature, hidden):
    #x --> caption integer sequence
    #defining attention model
    context_vector, attention_weights = self.attention(feature, hidden)
    # x shape after passing through embedding --> (batch_size, 1, embedding_dim)
    x = self.embedding(x)
    #now we need to pass input to the GRU and i/p is both context vector and x
    #so we need to concatenate them
    #context vector shape -->(batch_size, hidden_size) to concatenate with x we need to convert it into shape (batch_size, 1, hidden_size)
    #x shape after concatenation (batch_size, 1, embedding_dim + hidden_size)
    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)
    
    #passing the concatenated vector to GRU
    #output --> hidden_state of each timestep
    #state--> cell state of last timestep
    output, state = self.gru(x)
    #print("RNN output(hidden state) shape: ", output.shape)
    #acc. to research paper we will create deep output layer

    #shape == (batch_size, max_length, hidden_size) (hidden_size is equal to units which is dimension of GRU)
    x = self.fc1(output)
    #x shape --> (batch_size*max_length, hidden_size)
    x = tf.reshape(x, (-1, x.shape[2]))
    #batch_size --> no. of data point
    #max_length --> maximum no. of word in each caption
    #batch_size*max_length --> corresponding to each word we generate a vector and later length of each vector will be change to vocab_size
    
    #output shape = (batch_size*max_length, vocab_size)
    x = self.fc2(x)

    return x, state, attention_weights

  def reset_state(self, batch_size):
    return tf.zeros((batch_size, self.units))

class Caption:

  def __init__(self, checkpoint_path, pickle_path, embedding_dim=256, units=512, vocab_size=5001, attention_feature_shape=64):
    self.embedding_dim = embedding_dim
    self.units = units
    self.vocab_size = vocab_size
    self.attention_feature_shape = attention_feature_shape
    self.checkpoint_path = checkpoint_path
    self.pickle_path = pickle_path

    self.encoder = CNN_Encoder(self.embedding_dim)
    self.decoder = RNN_Decoder(self.embedding_dim, self.units, self.vocab_size)
    self.optimizer = tf.keras.optimizers.Adam()

    self.image_feature_extract_model = self.create_inception()
    self.word_index, self.index_word = self.load_pickle(self.pickle_path)

    self.ckpt = tf.train.Checkpoint(optimizer = self.optimizer, encoder = self.encoder, decoder = self.decoder)
    self.ckpt_manager = tf.train.CheckpointManager(self.ckpt, self.checkpoint_path, max_to_keep=5)
    self.ckpt.restore(self.checkpoint_path + "\ckpt-2").expect_partial()

  #to preprocess image
  def load_image(self, image_path):
    img = tf.keras.preprocessing.image.load_img(image_path, target_size=(299, 299))
    img = tf.keras.preprocessing.image.img_to_array(img)
    #print("Shape after reading: ", img.shape)
    #img = tf.image.resize(img, (299, 299))
    #print("shape after resizing: ", img.shape)
    img = tf.keras.applications.inception_v3.preprocess_input(img)
    #print("shape after preprocessing: ", img.shape)
    return img, image_path
  
  def create_inception(self):
    image_model = tf.keras.applications.InceptionV3(include_top=False, weights="imagenet")
    new_input = image_model.input
    hidden_layer = image_model.layers[-1].output

    return tf.keras.Model(new_input, hidden_layer)

  def load_pickle(self, path):
    with open(path, "rb") as temp:
      obj = pickle.load(temp)

    return obj["word_index"], obj["index_word"]

  def gen_caption(self, path, max_train_length=34):

    attention_plot = np.zeros((max_train_length, self.attention_feature_shape))
    hidden = self.decoder.reset_state(batch_size = 1)

    temp_input = tf.expand_dims(self.load_image(path)[0], 0)
    img_tensor_val = self.image_feature_extract_model(temp_input)
    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))

    features = self.encoder(img_tensor_val)
    dec_input = tf.expand_dims([self.word_index["<start>"]], 0)
    result = []

    for i in range(max_train_length):
      prediction, hidden, attention_weight = self.decoder(dec_input, features, hidden)

      attention_plot[i] = tf.reshape(attention_weight, (-1, )).numpy()
      predicted_id = np.argmax(prediction, 1)[0]
      #predicted_id = tf.random.categorical(prediction, 1)[0][0].numpy()
      result.append(self.index_word[predicted_id])

      if self.index_word[predicted_id] == "<end>":
        break

      dec_input = tf.expand_dims([predicted_id], 0)

    attention_plot = attention_plot[:len(result), :]

    return result, attention_plot

  def plot_attention(self, path, result, attention_plot):
    temp_image = np.array(Image.open(path))

    fig = plt.figure(figsize=(10, 10))
    len_result = len(result)

    for l in range(len_result):
      if result[l].isalpha():
        temp_att = np.resize(attention_plot[l], (8, 8))
        ax = fig.add_subplot(len_result//2, len_result//2, l+1)
        ax.set_title(result[l])
        img = ax.imshow(temp_image)
        ax.imshow(temp_att, cmap="gray", alpha=0.6, extent=img.get_extent())

    plt.tight_layout()
    plt.show()

def main(path):
  chk_path=".\\Attention8k\\checkpoints\\train"
  pick_path = r'.\Attention8k\tokenizer.pkl'

  #image_extension = image_url.split(".")[-1]
  #path = tf.keras.utils.get_file("immage."+image_extension, origin=image_url)
  cap = Caption(chk_path, pick_path)
  #try:
  result, attention_plot = cap.gen_caption(path)
  #except:
  #  return "Some error has been occurred"
  
  #cap.plot_attention(path, result, attention_plot)

  return " ".join(word for word in result if word.isalpha())

#main('/content/IMG_20190208_162126.jpg')

#main('/content/IMG_20190217_111329.jpg')

